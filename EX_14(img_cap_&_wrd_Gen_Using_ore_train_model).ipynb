{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiNhwAmPn3YZqI1D53fzFC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Singhrahul2511/NLP_All_Experiment/blob/main/EX_14(img_cap_%26_wrd_Gen_Using_ore_train_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Caption Generation"
      ],
      "metadata": {
        "id": "Qvn46t35xB1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aItmvk23xNSr",
        "outputId": "ade6a4ad-cf5b-4269-a894-1386d4e07e52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAKI2-2JwqdF",
        "outputId": "9203ba5e-6ab1-47b0-beef-d92083b7651e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption: a man wearing glasses and a tie\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load the pre-trained BLIP model and processor from Hugging Face\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Function to generate caption for a given image\n",
        "def generate_caption(image_path):\n",
        "    # Load and process the image\n",
        "    image = Image.open(image_path)\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate caption using the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs)\n",
        "\n",
        "    # Decode and return the generated caption\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/6bd6ad3f-53e0-432c-91ee-a070d6169fe1.png\"  # Removed extra spaces from the image path\n",
        "caption = generate_caption(image_path)\n",
        "print(f\"Generated Caption: {caption}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Generation"
      ],
      "metadata": {
        "id": "Pa_DISyrxeZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
        "import torch\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load pretrained tokenizer and model for summarization\n",
        "tokenizer = BertTokenizerFast.from_pretrained('mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization')\n",
        "model = EncoderDecoderModel.from_pretrained('mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization').to(device)\n",
        "\n",
        "def generate_summary(text):\n",
        "    # Cut off at BERT max length of 512 tokens\n",
        "    inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "    # Generate the summary\n",
        "    output = model.generate(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Decode the output into text\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example text to summarize\n",
        "text = \"\"\"The field of natural language processing (NLP) involves the interaction between computers and humans using natural language.\n",
        "It lies at the intersection of computer science, artificial intelligence, and linguistics. NLP focuses on enabling computers\n",
        "to understand, interpret, and generate human language in a valuable way. Recent advancements in machine learning have led to significant\n",
        "progress in NLP, enabling tasks like language translation, summarization, sentiment analysis, and conversational agents.\"\"\"\n",
        "\n",
        "# Generate summary\n",
        "summary = generate_summary(text)\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5rDkwPoxga9",
        "outputId": "fae5a632-0388-4b24-b39d-91c06ce75fe9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: the field of natural language processing ( nlp ) focuses on enabling computers to understand, interpret, and generate human language in a valuable way. recent advancements in machine learning have led to significant progress in nlp, enabling tasks like language translation, summarization, sentiment analysis, and conversational agents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBm-AuGQzX78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}